{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6095676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from CoExBO._utils import TensorManager\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.optim.fit import fit_gpytorch_mll_torch\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "tm = TensorManager()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_dims = 10                        # number of dimensions\n",
    "dataset = \"ranger9\"\n",
    "seed = 1            # random seed for reproduce the results #SEED 8 rpart_preproc, SEED 3 for Ranger, SEED 1 for rpart (50000_ckpt), 5 for rpart val, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a7ddd",
   "metadata": {},
   "source": [
    "## 1. Problem definition\n",
    "Let's get started with the toy example of two-dimensional Branin function.<br>\n",
    "More details can be found [here](https://www.sfu.ca/~ssurjano/branin.html)<br>\n",
    "- bounds: -2 from 3\n",
    "- variable type: continuous\n",
    "- ground truth X: [-1.02543108, -1.02543108]\n",
    "- ground truth Y: 10.6043\n",
    "- optimisation: maximisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbfb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark():\n",
    "    def __init__(self, X_data, Y_data, sample = False):\n",
    "        # self.X_data = X_data\n",
    "        # self.Y_data = Y_data\n",
    "        print(\"Training Model\")\n",
    "        if sample:\n",
    "            self.X_data, self.Y_data = self.sample_for_training(X_data, Y_data)\n",
    "        else:\n",
    "            self.X_data, self.Y_data = X_data, Y_data\n",
    "        self.gp_model = SingleTaskGP(torch.tensor(self.X_data), torch.tensor(self.Y_data)).cuda()\n",
    "        mll = ExactMarginalLogLikelihood(self.gp_model.likelihood, self.gp_model).cuda()\n",
    "        fit_gpytorch_mll(mll=mll)\n",
    "        print(\"Training Done\")\n",
    "\n",
    "    def find_closest_point(self, query, loop = False):\n",
    "        y_set = []\n",
    "        query = query.squeeze()\n",
    "        self.gp_model.eval()\n",
    "        if len(query.shape) > 1:\n",
    "            for X in query:\n",
    "                posterior = self.gp_model.posterior(X.unsqueeze(0).float().cuda())\n",
    "                mean = posterior.mean.item()\n",
    "                variance = posterior.variance.item()\n",
    "\n",
    "                # distances = np.linalg.norm(self.X_data - X, axis=1)\n",
    "                # closest_index = np.argmin(distances)\n",
    "                # y = self.Y_data[closest_index]\n",
    "                mean = np.clip(mean,0,1)\n",
    "                y_set.append(mean)\n",
    "            return torch.tensor(y_set, dtype=torch.float32).squeeze()\n",
    "        else:\n",
    "            X = query#.detach().numpy()\n",
    "            posterior = self.gp_model.posterior(X.unsqueeze(0).float().cuda())\n",
    "            mean = posterior.mean.item()\n",
    "            variance = posterior.variance.item()\n",
    "            mean = np.clip(mean,0,1)\n",
    "            # distances = np.linalg.norm(self.X_data - X, axis=1)\n",
    "            # closest_index = np.argmin(distances)\n",
    "            # y = self.Y_data[closest_index]\n",
    "            # print(\"X: \", X)\n",
    "            # distances = np.linalg.norm(self.X_data - X, axis=1)\n",
    "            # closest_index = np.argmin(distances)\n",
    "            # print(\"closest_index: \", closest_index)\n",
    "            # print(\"distances: \", distances[closest_index])\n",
    "            # y = self.Y_data[closest_index]\n",
    "            return torch.tensor(mean, dtype=torch.float32).reshape([1])\n",
    "\n",
    "    def sample_for_training(self, X, Y):\n",
    "        Y = Y.squeeze()\n",
    "        yuniq, ycount = np.unique(Y, return_counts=True)\n",
    "        # print(yuniq)\n",
    "        # print(ycount)\n",
    "        counts = {v: c for v, c in zip(yuniq, ycount)}\n",
    "\n",
    "        # print(counts)\n",
    "        # for i in range(len(Y)):\n",
    "        #     print(Y[i])\n",
    "        #     print(counts[Y[i]])\n",
    "        logits = np.array([Y[i] / counts[Y[i]] for i in range(len(Y))])\n",
    "        freq_idx = logits.argsort()[::-1]\n",
    "\n",
    "        selected_rows = freq_idx[:(3 * len(yuniq))]\n",
    "        np.random.shuffle(selected_rows)\n",
    "        X = X[selected_rows]\n",
    "        Y = Y[selected_rows]\n",
    "        #stdY = (Y - Y.mean()) / Y.std()\n",
    "\n",
    "        num_dims = list(np.arange(X.shape[-1]))\n",
    "        cat_dims = []\n",
    "\n",
    "        # Fit and save GP\n",
    "        print(f'Fit GP on dataset {dataset} containing {X.shape[0]} points...')\n",
    "        X_ = torch.from_numpy(X).to(dtype=torch.float64)\n",
    "        Y_ = torch.from_numpy(Y).to(dtype=torch.float64)\n",
    "        #print(\"Y SHAPE: \", Y.shape)\n",
    "        return X_, Y_.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b18edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit = 0\n",
    "upper_limit = 1\n",
    "colour_map = 'summer'\n",
    "resolution = 200\n",
    "ground_truth = torch.tensor([-1.02543108, -1.02543108])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3f8a0",
   "metadata": {},
   "source": [
    "We assume we have a prior knowledge about this function.<br>\n",
    "And we try to guide BO to rectify their recommendatation with help of explanation features.<br>\n",
    "<br>\n",
    "As we can see, the global maximum is at around [-1, -1].<br>\n",
    "We can also observe there are 9 peaks for this function.<br>\n",
    "We can expect BO can be stuck in one of these local maxima.<br>\n",
    "<br>\n",
    "Let's try to shepherd BO go to [-1, -1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b24407",
   "metadata": {},
   "source": [
    "## 2. Define domain\n",
    "Next, we define the domain of interest.\n",
    "We set domain as uniform distribution bounded -2 from 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbafa1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# set bounds\n",
    "\n",
    "mins = lower_limit * torch.ones(n_dims)\n",
    "maxs = upper_limit * torch.ones(n_dims)\n",
    "bounds = torch.vstack([mins, maxs]) # bounds\n",
    "\n",
    "# set domain\n",
    "from CoExBO._prior import Uniform    # Import prior from SOBER libraries\n",
    "domain = Uniform(bounds)\n",
    "\n",
    "# visualise domain\n",
    "samples = domain.sample(1000)\n",
    "# sns.pairplot(pd.DataFrame(tm.numpy(samples)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87eb557",
   "metadata": {},
   "source": [
    "## 3. Preferential learning\n",
    "Next, we try to learn the prior knowledge of human user.<br>\n",
    "For simplicity, human selection process is automatically generated with 60% accuracy via simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e81240d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CoExBO._coexbo import CoExBOwithSimulation, StateManager\n",
    "n_init_pref = 100      # number of initial random samples for preferential learning\n",
    "n_init_obj = 1        # number of initial random samples for objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c14bf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/ranger9/tnpa/760000_ckpt.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnpa\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(results_path, dataset, model_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m760000_ckpt.tar\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#rpart_50k was best #ranger9_v1: 290000,\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_from_training\u001b[39m(X_train, y_train, max_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/metaexbo/lib/python3.9/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/metaexbo/lib/python3.9/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/metaexbo/lib/python3.9/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/ranger9/tnpa/760000_ckpt.tar'"
     ]
    }
   ],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "def load_module(filename):\n",
    "    module_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    return SourceFileLoader(module_name, filename).load_module(module_name)\n",
    "import yaml\n",
    "model_type = 'tnpa'\n",
    "model_cls = getattr(load_module(f'./models/{model_type}.py'), model_type.upper())\n",
    "with open(f'configs/{dataset}/{model_type}.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "#if args.model in [\"np\", \"anp\", \"cnp\", \"canp\", \"bnp\", \"banp\", \"tnpa\", \"tnpd\", \"tnpnd\"]:\n",
    "model = model_cls(**config)\n",
    "model.cuda()\n",
    "import os.path as osp\n",
    "import torch\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from botorch.utils import t_batch_mode_transform\n",
    "from CoExBO._utils import TensorManager\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results_path = './results/'\n",
    "model_name = 'tnpa'\n",
    "ckpt_path = osp.join(results_path, dataset, model_name, '390000_ckpt.tar') #rpart_50k was best #ranger9_v1: 290000,\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt.model)\n",
    "\n",
    "def sample_from_training(X_train, y_train, max_points = 100):\n",
    "    samples_X = []\n",
    "    samples_y = []\n",
    "    for i in range(len(X_train)):\n",
    "        if len(X_train[i]) < max_points:\n",
    "            points_to_sample = len(X_train[i])\n",
    "        else:\n",
    "            points_to_sample = max_points\n",
    "        r_idx = np.random.randint(0,len(X_train[i]),points_to_sample)\n",
    "        samples_X.append(X_train[i][r_idx])\n",
    "        samples_y.append(y_train[i][r_idx])\n",
    "    return samples_X, samples_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb3d21",
   "metadata": {},
   "source": [
    "## 4. Run CoExBO\n",
    "Now, we will collaborate with BO for faster convergence.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b70b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50     # number of iterations\n",
    "\n",
    "\n",
    "# initial setting\n",
    "torch.manual_seed(seed)\n",
    "state = StateManager(n_dims=n_dims) # beta_init = 0.5 best working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e360b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n",
      "Training Model\n",
      "Training Done\n",
      "0) parameters: beta 2.828e+00 gamma 0.000e+00\n",
      "dataset_obj:  tensor([0.0336])\n",
      "Trajectory:  [tensor([0.0336]), 0.6958414912223816]\n",
      "1) parameters: beta 4.000e+00 gamma 1.000e-02\n",
      "dataset_obj:  tensor([0.0336, 0.6958])\n",
      "Trajectory:  [tensor([0.0336]), 0.6958414912223816, 0.8847091794013977]\n",
      "2) parameters: beta 4.899e+00 gamma 4.000e-02\n",
      "dataset_obj:  tensor([0.0336, 0.6958, 0.8847])\n",
      "Trajectory:  [tensor([0.0336]), 0.6958414912223816, 0.8847091794013977, 0.8965584635734558]\n",
      "3) parameters: beta 5.657e+00 gamma 9.000e-02\n",
      "dataset_obj:  tensor([0.0336, 0.6958, 0.8847, 0.8966])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m beta, gamma \u001b[38;5;241m=\u001b[39m state(t)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_obj: \u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_obj[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 64\u001b[0m result, dataset_obj, dataset_duel \u001b[38;5;241m=\u001b[39m \u001b[43mcoexbo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_duel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_TPN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlower_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_limit\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m best_v \u001b[38;5;241m=\u001b[39m dataset_obj[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_v \u001b[38;5;241m>\u001b[39m max_bv:\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_coexbo.py:673\u001b[0m, in \u001b[0;36mCoExBOwithSimulation.__call__\u001b[0;34m(self, dataset_obj, dataset_duel, beta, gamma, sigma, model_TPN, lower_limit, upper_limit)\u001b[0m\n\u001b[1;32m    671\u001b[0m         model\u001b[38;5;241m.\u001b[39mupper_limit \u001b[38;5;241m=\u001b[39m upper_limit\n\u001b[1;32m    672\u001b[0m         model\u001b[38;5;241m.\u001b[39mlower_limit \u001b[38;5;241m=\u001b[39m lower_limit\n\u001b[0;32m--> 673\u001b[0m     X_pairwise_next, dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_pairwise_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprior_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_models(X, Y, X_pairwise, y_pairwise)\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_coexbo.py:586\u001b[0m, in \u001b[0;36mCoExBOwithSimulation.generate_pairwise_candidates\u001b[0;34m(self, model, beta, prior_pref, gamma)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;66;03m# The other benchmarking acquisition function is not to learn human preference.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m     acqf \u001b[38;5;241m=\u001b[39m BaselineDuelingAcquisitionFunction(\n\u001b[1;32m    577\u001b[0m         model,\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m         raw_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_samples,\n\u001b[1;32m    584\u001b[0m     )\n\u001b[0;32m--> 586\u001b[0m X_pairwise_next \u001b[38;5;241m=\u001b[39m \u001b[43macqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m dist \u001b[38;5;241m=\u001b[39m (X_pairwise_next[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m X_pairwise_next[:,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_pairwise_next, dist\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_dueling_acquisition_function.py:197\u001b[0m, in \u001b[0;36mDuelingAcquisitionFunction.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m     X_suggest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonmyopic()\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdueling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     X_suggest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdueling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe method should be from [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonmyopic\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdueling\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_dueling_acquisition_function.py:181\u001b[0m, in \u001b[0;36mDuelingAcquisitionFunction.dueling\u001b[0;34m(self, meta)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# otherwise not.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     piucb\u001b[38;5;241m.\u001b[39mpi_augment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m X_pref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpiucb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m X_suggest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack([X_pref, X_ucb])\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_suggest\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_dueling_acquisition_function.py:60\u001b[0m, in \u001b[0;36mBaseDuelingAcquisitionFunction.optimize_function\u001b[0;34m(self, acqf, meta)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mOptimising the acquisition funtion to find the next query.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m- X_next: torch.tensor, the next query point\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta:\n\u001b[0;32m---> 60\u001b[0m     X_next \u001b[38;5;241m=\u001b[39m \u001b[43macqf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#X_next = torch.argmax(acqusition_fuction)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     X_next, _ \u001b[38;5;241m=\u001b[39m optimize_acqf(\n\u001b[1;32m     64\u001b[0m         acqf,\n\u001b[1;32m     65\u001b[0m         bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m         raw_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_samples,\n\u001b[1;32m     69\u001b[0m     )\n",
      "File \u001b[0;32m~/hnap/lib/python3.10/site-packages/botorch/utils/transforms.py:273\u001b[0m, in \u001b[0;36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorated\u001b[39m(\n\u001b[1;32m    268\u001b[0m     acqf: AcquisitionFunction, X: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# Allow using acquisition functions for other inputs (e.g. lists of strings)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, Tensor):\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(acqf)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires X to have at least 2 dimensions,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but received X with only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_acquisition_function.py:312\u001b[0m, in \u001b[0;36mCoExBO_UCB_Meta.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# print(\"likelihood_gp_std before: \", likelihood_gp_std)\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# print(\"self.beta.sqrt(): \", self.beta.sqrt())\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#likelihood_gp_mean, likelihood_gp_std = self._mean_and_sigma(X)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_augment:\n\u001b[0;32m--> 312\u001b[0m     posterior_gp_mean, posterior_gp_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood_gp_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood_gp_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     aqcf \u001b[38;5;241m=\u001b[39m posterior_gp_mean \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m*\u001b[39m posterior_gp_std\n\u001b[1;32m    314\u001b[0m     point \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(aqcf)\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_acquisition_function.py:267\u001b[0m, in \u001b[0;36mCoExBO_UCB_Meta.posterior_gp\u001b[0;34m(self, X, likelihood_gp_mean, likelihood_gp_std)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mposterior_gp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, likelihood_gp_mean, likelihood_gp_std):\n\u001b[0;32m--> 267\u001b[0m     prior_gp_mean, prior_gp_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     prior_gp_std_max \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m likelihood_gp_std\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m prior_gp_std\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    270\u001b[0m     )\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    271\u001b[0m     posterior_gp_std \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    272\u001b[0m         prior_gp_std_max\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m likelihood_gp_std\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m    273\u001b[0m             prior_gp_std_max\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m likelihood_gp_std\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m     )\u001b[38;5;241m.\u001b[39msqrt()\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_acquisition_function.py:258\u001b[0m, in \u001b[0;36mCoExBO_UCB_Meta.prior_gp\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprior_gp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 258\u001b[0m     prior_mean, prior_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior_pref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     prior_mean_conv \u001b[38;5;241m=\u001b[39m (prior_mean \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE_y_pref) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_y_pref \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_y_obs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE_y_obs\n\u001b[1;32m    261\u001b[0m     prior_std_conv \u001b[38;5;241m=\u001b[39m prior_std \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_y_pref \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_y_obs\n",
      "File \u001b[0;32m/lustre/lunari/NPF/CoExBO-meta/CoExBO/_monte_carlo_quadrature.py:114\u001b[0m, in \u001b[0;36mMonteCarloQuadrature.probability\u001b[0;34m(self, X, mean, both)\u001b[0m\n\u001b[1;32m    112\u001b[0m n_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresh)\n\u001b[1;32m    113\u001b[0m Xs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(X, chunks\u001b[38;5;241m=\u001b[39mn_chunks, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft_copeland_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m Y_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack([result[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_chunks)])\n\u001b[1;32m    118\u001b[0m Y_std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack([result[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_chunks)])\n",
      "File \u001b[0;32m~/hnap/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hnap/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/hnap/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "noise_level = 0.2\n",
    "import numpy as np\n",
    "meta = True\n",
    "X_test = np.load(f'./datasets/{dataset}/X_test.npz')\n",
    "y_test = np.load(f'./datasets/{dataset}/y_test.npz')\n",
    "X_validation = np.load(f'./datasets/{dataset}/X_validation.npz')\n",
    "y_validation = np.load(f'./datasets/{dataset}/y_validation.npz')\n",
    "\n",
    "X_test_ = np.array([X_test[key].astype('float32') for key in X_test.keys()])\n",
    "y_test_ = np.array([y_test[key].astype('float32') for key in y_test.keys()])\n",
    "\n",
    "\n",
    "X_validation_ = np.array([X_validation[key].astype('float32') for key in X_validation.keys()])\n",
    "y_validation_ = np.array([y_validation[key].astype('float32') for key in y_validation.keys()])\n",
    "\n",
    "#FLIP\n",
    "# X_validation_ = np.array([X_test[key].astype('float32') for key in X_test.keys()])\n",
    "# y_validation_ = np.array([y_test[key].astype('float32') for key in y_test.keys()])\n",
    "\n",
    "\n",
    "# X_test_ = np.array([X_validation[key].astype('float32') for key in X_validation.keys()])\n",
    "# y_test_ = np.array([y_validation[key].astype('float32') for key in y_validation.keys()])\n",
    "\n",
    "samples_val_X, samples_val_y = sample_from_training(X_validation_, y_validation_, max_points=5)\n",
    "\n",
    "samples_val_X = torch.tensor(np.vstack(samples_val_X))\n",
    "samples_val_y = torch.tensor(np.vstack(samples_val_y))\n",
    "samples_val_X = samples_val_X + np.random.normal(0, noise_level, size=samples_val_X.shape)\n",
    "samples_val_y = samples_val_y + np.random.normal(0, noise_level, size=samples_val_y.shape)\n",
    "\n",
    "#X_t_fit, y_t_fit = sample_from_training(X_test_,y_test_, max_points = 5000)\n",
    "best_values = []\n",
    "total_trajectories = []\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    torch.random.manual_seed(seed)\n",
    "    #X_t_fit, y_test_ = sample_from_training(X_test_[i],y_test_[i], max_points = 1000)\n",
    "\n",
    "    benchmark = Benchmark(X_test_[i], y_test_[i], sample=False)\n",
    "    # print(y_test[i])\n",
    "    benchmark2 = Benchmark(samples_val_X, samples_val_y)\n",
    "    true_function = benchmark.find_closest_point\n",
    "    model.test_set_X = X_test_[i]\n",
    "    min_test_idx = np.argmin(y_test_[i])\n",
    "    #print(\"y_test_[i]: \", y_test_[i][min_test_idx])\n",
    "    x_val = torch.tensor(X_test_[i][min_test_idx]).unsqueeze(0)\n",
    "    y_val = true_function(x_val)\n",
    "    dataset_obj = (x_val,y_val)\n",
    "    \n",
    "    true_function2 = benchmark2.find_closest_point\n",
    "    coexbo = CoExBOwithSimulation(domain, true_function, sigma=0.1, hallucinate=False, meta = meta)\n",
    "    coexbo2 = CoExBOwithSimulation(domain, true_function2, sigma=0.1, hallucinate=False, meta = meta)\n",
    "    #dataset_obj, _ = coexbo.initial_sampling(n_init_obj, n_init_pref)\n",
    "    _, dataset_duel = coexbo2.initial_sampling(n_init_obj, n_init_pref)\n",
    "    # print(\"dataset_duel: \", dataset_duel)\n",
    "    #print(dataset_duel)\n",
    "    #_, dataset_duel = coexbo.initial_sampling(n_init_obj, n_init_pref)\n",
    "    max_bv = -1\n",
    "    trajectory = [y_val]\n",
    "    for t in range(n_iterations):\n",
    "        beta, gamma = state(t)\n",
    "        print(\"dataset_obj: \", dataset_obj[1])\n",
    "        result, dataset_obj, dataset_duel = coexbo(\n",
    "            dataset_obj, dataset_duel, beta, gamma, model_TPN = model, lower_limit= lower_limit, upper_limit=upper_limit\n",
    "        )\n",
    "        best_v = dataset_obj[1].max().item()\n",
    "        if best_v > max_bv:\n",
    "            max_bv = best_v\n",
    "        trajectory.append(max_bv)\n",
    "        print(\"Trajectory: \", trajectory)\n",
    "        #print(f\"{len(dataset_obj[0])}) Best value: {best_v:.5e}\")\n",
    "        results.append(result)\n",
    "    total_trajectories.append(dataset_obj[1].detach().cpu().numpy())\n",
    "    best_values.append(trajectory)\n",
    "results = torch.tensor(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the object as a pickle file \n",
    "with open(f'evaluations/meta_{dataset}_v3.pkl', 'wb') as file: #rpart v1 evaluated on ckpt 50000, best one so far v2 best evaluation for rpart #ranger9 v2 100k, v3 100k flip\n",
    "    pickle.dump(total_trajectories, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b615b72",
   "metadata": {},
   "source": [
    "# Results\n",
    "[overhead(s), best observation, Euclidean distance between the pairwise candidates, correct_answer_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260c915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([0.1787]),\n",
       "  0.4299771189689636,\n",
       "  0.5508123636245728,\n",
       "  0.5652381181716919,\n",
       "  0.5652381181716919,\n",
       "  0.5652381181716919,\n",
       "  0.5652381181716919,\n",
       "  0.5652381181716919,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155,\n",
       "  0.5719722509384155],\n",
       " [tensor([0.0991]),\n",
       "  0.3288358747959137,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255,\n",
       "  0.4815559685230255]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c76eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
